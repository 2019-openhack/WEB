{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 9/9 [00:02<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== success ====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'2016 IEEE Conference on Computer Vision and Pattern Recognition SingleImage Crowd Counting via MultiColumn Convolutional Neural Network Yingying Zhang Desen Zhou Siqin Chen Shenghua Gao Yi Ma {zhangyy2,zhouds,chensq,gaoshh,mayi}@shanghaitech.edu.cn Shanghaitech University Abstract This paper aims to develop a method than can accurately estimate the crowd count from an individual image with arbitrary crowd density and arbitrary perspective', b'To this end, we have proposed a simple but effective Multicolumn Convolutional Neural Network (MCNN) architecture to map the image to its crowd density map', b'The proposed MCNN allows the input image to be of arbitrary size or resolution', b'By utilizing lters with receptive elds of different sizes, the features learned by each column CNN are adaptive to variations in people/head size due to perspective effect or image resolution', b'Furthermore, the true density map is computed accurately based on geometryadaptive kernels which do not need knowing the perspective map of the input image', b'Since exiting crowd counting datasets do not adequately cover all the challenging situations considered in our work, we have collected and labelled a large new dataset that includes 1198 images with about 330,000 heads annotated', b'On this challenging new dataset, as well as all existing datasets, we conduct extensive experiments to verify the effectiveness of the proposed model and method', b'In particular, with the proposed simple MCNN model, our method outperforms all existing methods', b'In addition, experiments show that our model, once trained on one dataset, can be readily transferred to a new dataset', b'1', b'Introduction In the new year eve of 2015, 35 people were killed in a massive stampede in Shanghai, China', b'Unfortunately, since then, many more massive stampedes have taken place around the world which have claimed many more victims', b'Accurately estimating crowds from images or videos has become an increasingly important application of computer vision technology for purposes of crowd control and public safety', b'In some scenarios, such as public rallies and sports events, the number or density of participating people is an essential piece of information for future event planning and space design', b'Good methods of crowd counting can also be extended to other domains, for instance, counting cells or bacteria from microscopic images, animal crowd estimates in wildlife sanctuaries, or estimating the number of vehicles at transportation hubs or trafc jams, etc', b'Related work', b'Many algorithms have been proposed in the literature for crowd counting', b'Earlier methods [29] adopt a detectionstyle framework that scans a detector over two consecutive frames of a video sequence to estimate the number of pedestrians, based on boosting appearance and motion features', b'[19, 30, 31] have used a similar detectionbased framework for pedestrian counting', b'In detectionbased crowd counting methods, people typically assume a crowd is composed of individual entities which can be detected by some given detectors [13, 34, 18, 10]', b'The limitation of such detectionbased methods is that occlusion among people in a clustered environment or in a very dense crowd signicantly affects the performance of the detector hence the nal estimation accuracy', b'In counting crowds in videos, people have proposed to cluster trajectories of tracked visual features', b'For instance, [24] has used highly parallelized version of the KLT tracker and agglomerative clustering to estimate the number of moving people', b'[3] has tracked simple image features and probabilistically group them into clusters representing independently moving entities', b'However, such trackingbased methods do not work for estimating crowds from individual still images', b'Arguably the most extensively used method for crowd counting is featurebased regression, see [4, 7, 5, 27, 15, 20]', b'The main steps of this kind of method are: 1) segmenting the foreground; 2) extracting various features from the foreground, such as area of crowd mask [4, 7, 27, 23], edge count [4, 7, 27, 25], or texture features [22, 7]; 3) utilizing a regression function to estimate the crowd count', b'Linear [23] or piecewise linear [25] functions are relatively simple models and yield decent performance', b'Other more advanced/effective methods are ridge regression (RR) [7], Gaussian process regression (GPR) [4], and neural network (NN) [22]', b'There have also been some works focusing on crowd counting from still images', b'[12] has proposed to leverage 10636919/16 $31.00  2016 IEEE DOI 10.1109/CVPR.2016.70 589 multiple sources of information to compute an estimate of the number of individuals present in an extremely dense crowd visible in a single image', b'In that work, a dataset of fty crowd images containing 64K annotated humans (UCF CC 50) is introduced', b'[2] has followed the work and estimated counts by fusing information from multiple sources, namely, interest points (SIFT), Fourier analysis, wavelet decomposition, GLCM features, and low condence head detections', b'[28] has utilized the features extracted from a pretrained CNN to train a support vector machine (SVM) that subsequently generates counts for still images', b'Recently Zhang et al', b'[33] has proposed a CNN based method to count crowd in different scenes', b'They rst pretrain a network for certain scenes', b'When a test image from a new scene is given, they choose similar training data to netune the pretrained network based on the perspective information and similarity in density map', b'Their method demonstrates good performance on most existing datasets', b'But their method requires perspective maps both on training scenes and the test scene', b'Unfortunately, in many practical applications of crowd counting, the perspective maps are not readily available, which limits the applicability of such methods', b'Contributions of this paper', b'In this paper, we aim to conduct accurate crowd counting from an arbitrary still image, with an arbitrary camera perspective and crowd density (see Figure 1 for some typical examples)', b'At rst sight this seems to be a rather daunting task, since we obviously need to conquer series of challenges: 1', b'Foreground segmentation is indispensable in most existing work', b'However foreground segmentation is a challenging task all by itself and inaccurate segmentation will have irreversible bad effect on the nal count', b'In our task, the viewpoint of an image can be arbitrary', b'Without information about scene geometry or motion, it is almost impossible to segment the crowd from its background accurately', b'Hence, we have to estimate the number of crowd without segmenting the foreground rst', b'2', b'The density and distribution of crowd vary signicantly in our task (or datasets) and typically there are tremendous occlusions for most people in each image', b'Hence traditional detectionbased methods do not work well on such images and situations', b'3', b'As there might be signicant variation in the scale of the people in the images, we need to utilize features at different scales all together in order to accurately estimate crowd counts for different images', b'Since we do not have tracked features and it is difcult to handcraft features for all different scales, we have to resort (a) (b) Figure 1: (a) Representative images of Part A in our new crowd dataset', b'(b) Representative images of Part B in our crowd dataset', b'All faces are blurred in (b) for privacy preservation', b'to methods that can automatically learn effective features', b'To overcome above challenges, in this work, we propose a novel framework based on convolutional neural network (CNN) [9, 16] for crowd counting in an arbitrary still image', b'More specically, we propose a multicolumn convolutional neural network (MCNN) inspired by the work of [8], which has proposed multicolumn deep neural networks for image classication', b'In their model, an arbitrary number of columns can be trained on inputs preprocessed in different ways', b'Then nal predictions are obtained by averaging individual predictions of all deep neural networks', b'Our MCNN contains three columns of convolutional neural networks whose lters have different sizes', b'Input of the MCNN is the image, and its output is a crowd density map whose integral gives the overall crowd count', b'Contributions of this paper are summarized as follows: 1', b'The reason for us to adopt a multicolumn architecture here is rather natural: the three columns correspond to lters with receptive elds of different sizes (large, medium, small) so that the features learned by each column CNN is adaptive to (hence the overall network is robust to) large variation in people/head size due to perspective effect or across different image resolutions', b'2', b'In our MCNN, we replace the fully connected layer with a convolution layer whose lter size is 1  1', b'Therefore the input image of our model can be of arbitrary size to avoid distortion', b'The immediate output of the network is an estimate of the density of the crowd from which we derive the overall count', b'3', b'We collect a new dataset for evaluation of crowd 590 counting methods', b'Existing crowd counting datasets cannot fully test the performance of an algorithm in the diverse scenarios considered by this work because their limitations in the variation in viewpoints (UCSD, WorldExpo10), crowd counts (UCSD), the scale of dataset (UCSD, UCF CC 50), or the variety of scenes (UCF CC 50)', b'In this work we introduce a new largescale crowd dataset named Shanghaitech of nearly 1,200 images with around 330,000 accurately labeled heads', b'As far as we know, it is the largest crowd counting dataset in terms of number annotated heads', b'No two images in this dataset are taken from the same viewpoint', b'This dataset consists of two parts: Part A and Part B', b'Images in Part A are randomly crawled from the Internet, most of them have a large number of people', b'Part B are taken from busy streets of metropolitan areas in Shanghai', b'We have manually annotated both parts of images and will share this dataset by request', b'Figure 1 shows some representative samples of this dataset', b'2', b'Multicolumn CNN for Crowd Counting 2.1', b'Density map based crowd counting To estimate the number of people in a given image via the Convolutional Neural Networks (CNNs), there are two natural congurations', b'One is a network whose input is the image and the output is the estimated head count', b'The other one is to output a density map of the crowd (say how many people per square meter), and then obtain the head count by integration', b'In this paper, we are in favor of the second choice for the following reasons: 1', b'Density map preserves more information', b'Compared to the total number of the crowd, density map gives the spatial distribution of the crowd in the given image, and such distribution information is useful in many applications', b'For example, if the density in a small region is much higher than that in other regions, it may indicate something abnormal happens there', b'2', b'In learning the density map via a CNN, the learned lters are more adapted to heads of different sizes, hence more suitable for arbitrary inputs whose perspective effect varies signicantly', b'Thus the lters are more semantic meaningful, and consequently improves the accuracy of crowd counting', b'2.2', b'Density map via geometryadaptive kernels Since the CNN needs to be trained to estimate the crowd density map from an input image, the quality of density given in the training data very much determines the performance of our method', b'We rst describe how to convert an image with labeled people heads to a map of crowd density', b'If there is a head at pixel xi, we represent it as a delta function (x  xi)', b'Hence an image with N heads labeled can be represented as a function N(cid:2) H(x) = (x  xi)', b'i=1 To convert this to a continuous density function, we may convolve this function with a Gaussian kernel[17] G so that the density is F (x) = H(x)  G(x)', b'However, such a density function assumes that these xi are independent samples in the image plane which is not the case here: In fact, each xi is a sample of the crowd density on the ground in the 3D scene and due to the perspective distortion, and the pixels associated with different samples xi correspond to areas of different sizes in the scene', b'Therefore, to accurately estimate the crowd density F , we need to take into account the distortion caused by the homography between the ground plane and the image plane', b'Unfortunately, for the task (and datasets) at hand, we typically do not know the geometry of the scene', b'Nevertheless, if we assume around each head, the crowd is somewhat evenly distributed, then the average distance between the head and its nearest k neighbors (in the image) gives a reasonable estimate of the geometric distortion (caused by the perspective effect)', b'Therefore, we should determine the spread parameter  based on the size of the head for each person within the image', b'However, in practice, it is almost impossible to accurately get the size of head due to the occlusion in many cases, and it is also difcult to nd the underlying relationship between the head size the density map', b'Interesting we found that usually the head size is related to the distance between the centers of two neighboring persons in crowded scenes (please refer to Figure 2)', b'As a compromise, for the density maps of those crowded scenes, we propose to dataadaptively determine the spread parameter for each person based on its average distance to its neighbors.1 For each head xi in a given image, we denote the dis(cid:3)m tances to its k nearest neighbors as {di m}', b'The 2, ', b'', b'', b', di 1, di average distance is therefore di = 1 j=1 di j', b'Thus, m the pixel associated with xi corresponds to an area on the ground in the scene roughly of a radius proportional to di', b'Therefore, to estimate the crowd density around the pixel xi, we need to convolve (x  xi) with a Gaussian kernel with variance i proportional to di, More precisely, the 1For the images given the density or perspective maps, we directly use the given density maps in our experiments or use the density maps generated from perspective maps', b'For those data only contain very few persons and the sizes of heads are similar, we use the xed spread parameter for all the persons', b'591 density F should be N(cid:2) F (x) = (x  xi)  Gi(x), with i =  di i=1 for some parameter ', b'In other words, we convolve the labels H with density kernels adaptive to the local geometry around each data point, referred to as geometryadaptive kernels', b'In our experiment, we have found empirically  = 0.3 gives the best result', b'In Figure 2, we have shown soobtained density maps of two exemplar images in our dataset', b'Figure 2: Original images and corresponding crowd density maps obtained by convolving geometryadaptive Gaussian kernels', b'2.3', b'Multicolumn CNN for density map estimation Due to perspective distortion, the images usually contain heads of very different sizes, hence lters with receptive elds of the same size are unlikely to capture characteristics of crowd density at different scales', b'Therefore, it is more natural to use lters with different sizes of local receptive eld to learn the map from the raw pixels to the density maps', b'Motivated by the success of Multicolumn Deep Neural Networks (MDNNs) [8], we propose to use a Multicolumn CNN (MCNN) to learn the target density maps', b'In our MCNN, for each column, we use the lters of different sizes to model the density maps corresponding to heads of different scales', b'For instance, lters with larger receptive elds are more useful for modeling the density maps corresponding to larger heads', b'  \\x17\\x17 /\\x17    W\\x17 \\x17\\x17  W\\x17 \\x17\\x17  \\x17\\x17  W\\x17 \\x17\\x17  W\\x17 \\x17\\x17  \\x17\\x17  W\\x17 \\x17\\x17  W\\x17 \\x17\\x17  \\x17\\x17  \\x17\\x17 D\\x17 \\x17 \\x17\\x17  Figure 3: The structure of the proposed multicolumn convolutional neural network for crowd density map estimation', b'The overall structure of our MCNN is illustrated in Figure 3', b'It contains three parallel CNNs whose lters are with local receptive elds of different sizes', b'For simplication, we use the same network structures for all columns (i.e., convpoolingconvpooling) except for the sizes and numbers of lters', b'Max pooling is applied for each 2 2 region, and Rectied linear unit (ReLU) is adopted as the activation function because of its good performance for CNNs [32]', b'To reduce the computational complexity (the number of parameters to be optimized), we use less number of lters for CNNs with larger lters', b'We stack the output feature maps of all CNNs and map them to a density map', b'To map the features maps to the density map, we adopt lters whose sizes are 1  1 [21]', b'Then Euclidean distance is used to measure the difference between the estimated density map and ground truth', b'The loss function is dened as follows: L() = 1 2N (cid:3)F (Xi; )  Fi(cid:3)2 2, (1) N(cid:2) i=1 where  is a set of learnable parameters in the MCNN', b'N is the number of training image', b'Xi is the input image and Fi is the ground truth density map of image Xi', b'F (Xi; ) stands for the estimated density map generated by MCNN which is parameterized with  for sample Xi', b'L is the loss between estimated density map and the ground truth density map', b'Remarks i) Since we use two layers of max pooling, the spatial resolution is reduced by 1 4 for each image', b'So in the training stage, we also downsample each training sample by 1 4 before generating its density map', b'ii) Conventional CNNs usually normalize their input images to the same size', b'Here we prefer the input images to be of their original sizes because resizing images to the same size will introduce additional distortion in the density map that is difcult to estimate', b'iii) Besides the fact that the lters have different sizes in our CNNs, another difference between our MCNN and conventional MDNNs is that we combine the outputs of all CNNs with learnable weights (i.e.,11 lters)', b'In contrast, in MDNNs proposed by [8], the outputs are simply averaged', b'2.4', b'Optimization of MCNN The loss function (1) can be optimized via batchbased stochastic gradient descent and backpropagation, typical for training neural networks', b'However, in reality, as the number of training samples are very limited, and the effect of gradient vanishing for deep neural networks, it is not easy to learn all the parameters simultaneously', b'Motivated by the success of pretraining of RBM [11], we pretrain CNN in each single column separately by directly mapping the outputs of the fourth convolutional layer to the density map', b'We then use these pretrained CNNs to initialize CNNs in all columns and netune all the parameters simultaneously', b'592 2.5', b'Transfer learning setting One advantage of such a MCNN model for density estimation is that the lters are learned to model the density maps of heads with different sizes', b'Thus if the model is trained on a large dataset which contains heads of very different sizes, then the model can be easily adapted (or transferred) to another dataset whose crowd heads are of some particular sizes', b'If the target domain only contains a few training samples, we may simply x the rst several layers in each column in our MCNN, and only netune the last few convolutional layers', b'There are two advantages for netuning the last few layers in this case', b'Firstly, by xing the rst several layers, the knowledge learnt in the source domain can be preserved, and by netuning the last few layers, the models can be adapted to the target domain', b'So the knowledge in both source domain and target domain can be integrated and help improve the accuracy', b'Secondly, comparing with netuning the whole network, netuning the last few layers greatly reduces the computational complexity', b'3', b'Experiments We evaluate our MCNN model on four different datasets  three existing datasets and our own dataset', b'Although comparing to most DNN based methods in the literature, the proposed MCNN model is not particularly deep nor sophisticated, it has nevertheless achieved competitive and often superior performance in all the datasets', b'In the end, we also demonstrate the generalizability of such a simple model in the transfer learning setting (as mentioned in section 2.5)', b'Implementation of the proposed network and its training are based on the Caffe framework developed by [14]', b'3.1', b'Evaluation metric By following the convention of existing works [28][33] for crowd counting, we evaluate different methods with both the absolute error (MAE) and the mean squared error (MSE), which are dened as follows: M AE = N(cid:2) 1 1 N |zi  zi|, M SE = (cid:4)(cid:5)(cid:5)(cid:6) 1 N(cid:2) N 1 (zi  zi)2 (2) where N is the number of test images, zi is the actual number of people in the ith image, and zi is the estimated number of people in the ith image', b'Roughly speaking, M AE indicates the accuracy of the estimates, and M SE indicates the robustness of the estimates', b'3.2', b'Shanghaitech dataset As exiting datasets are not entirely suitable for evaluation of the crowd count task considered in this work, we 593 introduce a new largescale crowd counting dataset named Shanghaitech which contains 1198 annotated images, with a total of 330,165 people with centers of their heads annotated', b'As far as we know, this dataset is the largest one in terms of the number of annotated people', b'This dataset consists of two parts: there are 482 images in Part A which are randomly crawled from the Internet, and 716 images in Part B which are taken from the busy streets of metropolitan areas in Shanghai', b'The crowd density varies signicantly between the two subsets, making accurate estimation of the crowd more challenging than most existing datasets', b'Both Part A and Part B are divided into training and testing: 300 images of Part A are used for training and the remaining 182 images for testing;, and 400 images of Part B are for training and 316 for testing', b'Table1 gives the statistics of Shanghaitech dataset and its comparison with other datasets', b'We also give the crowd histograms of images in this dataset in Figure 4', b'If the work is accepted for publication, we will release the dataset, the annotations, as well as the training/testing protocol', b'Part_A Part_B 250 200 150 100 50 s e g a m I f o r e b m u N 0 0 250 200 150 100 50 s e g a m I f o r e b m u N 3500 0 0 500 1000 1500 2000 2500 3000 Number of People in the Image 100 200 300 400 500 600 Number of People in the Image Figure 4: Histograms of crowd counts of our new dataset', b'To augment the training set for training the MCNN, we cropped 9 patches from each image at different locations, and each patch is 1/4 size of the original image', b'All the patches are used to train our MCNN model', b'For Part A, as the crowd density is usually very high, we use our geometryadaptive kernels to generate the density maps, and the predicted density at overlapping region is calculated by averaging', b'For Part B, since the crowd is relatively sparse, we use the same spread in Gaussian kernel to generate the (ground truth) density maps', b'In our implementation, we rst pretrain each column of MCNN independently', b'Then we netune the whole network', b'Figure 5 shows examples of ground truth density maps and estimated density maps of images in Part A', b'We compare our method with the work of Zhang et al', b'[33], which also uses CNNs for crowd counting and achieved stateoftheart accuracy at the time', b'Following the work of [33], we also compare our work with regression based method, which uses Local Binary Pattern (LBP) features extracted from the original image as input and uses ridge regression (RR) to predict the crowd number for each image', b'To extract LBP features, each image is uniformly Table 1: Comparation of Shanghaitech dataset with existing datasets: Num is the number of images; Max is the maximual crowd count; Min is the minimal crowd count; Ave is the average crowd count; Total is total number of labeled people', b'Dataset UCSD UCF CC 50 WorldExpo Resolution Num Max Min 158  238 11 94 different 576  720 1 33 different Part A Part B 768  1024 9 2000 50 3980 482 716 46 4543 253 3139 578 Shanghaitech Ave 24.9 1279.5 50.2 501.4 123.6 Total 49,885 63,974 199,923 241,677 88,488 Test image Groundtruth Estimation Test image Groundtruth Estimation Figure 5: The ground truth density map and estimated density map of our MCNN Model of two test images in part A divided into 8  8 blocks in Part A and 12  16 blocks in Part B, then a 59dimensional uniform LBP in each block is extracted and all uniform LBP features are concatenated together to represent the image', b'The ground truth is a 64D or 192D vector where each entry is the total number of persons in corresponding patch', b'We compare the performances of all the methods on Shanghaitech dataset in Table 2', b'The effect of pretraining in MCNN', b'We show the effect of our model without pretraining on Shanghaitech dataset Part A in Figure 6', b'We see that pretrained network outperforms the network without pretraining', b'The result veries the necessity of pretraining for MCNN as optimization starting from random initialization tends to fall into local minima', b'Single column CNNs vs MCNN', b'Figure 6 shows the comparison of single column CNNs with MCNN on Shanghaitech dataset Part A', b'It can be seen that MCNNs signicantly outperforms each single column CNN for both MAE and MSE', b'This veries the effectiveness of the MCNN architecture', b'                 D\\x17\\x17 D^\\x17   \\x17EE> \\x17\\x17EED \\x17\\x17EE^ D\\x17EE\\x17\\x17 D\\x17EE Figure 6: Comparing single column CNNs with MCNN and MCNN w/o pretraining on Part A', b'L, M, S stand for large kernel, medium kernel, small kernel respectively', b'Comparison of different loss functions', b'We evaluate the performance of our framework with different loss functions', b'Other than mapping the images to their density maps, we can also map the images to the total head counts in the image directly', b'For the input image Xi (i = 1, ', b'', b'', b', N), its total head count is zi, and F (Xi; ) stands for the estimated density map and  is the parameters of MCNN', b'Then we arrive the following objective function: L() = 1 2N (cid:8)(cid:8) N(cid:2) (cid:7)(cid:7)(cid:7) i=1 S F (Xi; )dxdy  zi (3) (cid:7)(cid:7)(cid:7)2 Here S stands for the spatial region of estimated density map, and ground truth of the density map is not used', b'For this loss, we also pretrain CNNs in each column separately', b'We call such a baseline as MCNN based crowd count regression (MCNNCCR)', b'Performance based on such loss function is listed in Table 2, which is also compared with two existing methods as well as the method based on density map estimation (simply labeled as MCNN)', b'We see that the results based on crowd count regression is rather poor', b'In a way, learning density map manages to preserve more information of the image, and subsequently helps improve the count accuracy', b'In Figure 7, we compare the results of our method with those of Zhang et al', b'[33] in more details', b'We group the test images in Part A and Part B into 10 groups according to crowd counts in an increasing order', b'We have 182+316 test images in Part A and Part B', b'Except for the 10th group which contains 20+37 images, other groups all have 18+31 images each', b'From the plots in the gure, we can see that our method is much more accurate and robust to large variation in crowd number/density', b'594 Table 2: Comparing performances of different methods on Shanghaitech dataset', b'Method LBP+RR Zhang et al', b'[33] MCNNCCR MCNN Part_A ground_truth MCNN Zhang et al', b'1400 1200 1000 800 600 400 200 s t n u o C e t l u o s b A Part A Part B MAE MSE MAE MSE 81.7 303.2 49.8 181.8 95.9 245.0 110.2 41.3 371.0 277.7 336.1 173.2 59.1 32.0 70.9 26.4 Part_B ground_truth MCNN Zhang et al', b'350 300 250 200 150 100 50 s t n u o C e t l u o s b A 0 1 2 3 4 6 5 Group ID 7 8 9 10 0 1 2 3 4 6 5 Group ID 7 8 9 10 Figure 7: Comparison of our method to Zhang et al', b'[33] on Shanghaitech dataset: We evenly divided our test images into 10 groups according to increasing number of people', b'Absolute count in the vertical axis is the average crowd number of images in each group', b'3.3', b'The UCF CC 50 dataset The UCF CC 50 dataset is rstly introduced by H', b'Idrees et al', b'[12]', b'This dataset contains 50 images from the Internet', b'It is a very challenging dataset, because of not only limited number of images, but also the crowd count of the image changes dramatically', b'The head counts range between 94 and 4543 with an average of 1280 individuals per image', b'The authors provided 63974 annotations in total for these fty images', b'We perform 5fold crossvalidation by following the standard setting in [12]', b'The same data augmentation approach as in that in Shanghaitech dataset', b'Table 3: Comparing results of different methods on the UCF CC 50 dataset', b'Method Rodriguez et al', b'[26] Lempitsky et al', b'[17] Idrees et al', b'[12] Zhang et al', b'[33] MCNN MAE MSE 697.8 655.7 487.1 493.4 541.6 419.5 467.0 498.5 377.6 509.1 We compare our method with four existing methods on UCF CC 50 dataset in Table 3', b'Rodriguez et al', b'[26] employs density map estimation to obtain better head detection results in crowd scenes', b'Lempitsky et al', b'[17] adopts 595 dense SIFT features on randomly selected patches and the MESA distance to learn a density regression model', b'The method presented in [12] gets the crowd count estimation by using multisource features', b'The work of Zhang et al', b'[33] is based on crowd CNN model to estimate the crowd count of an image', b'Our method achieves the best MAE, and comparable MSE with existing methods', b'3.4', b'The UCSD dataset We also evaluate our method on the UCSD dataset [4]', b'This dataset contains 2000 frames chosen from one surveillance camera in the UCSD campus', b'The frame size is 158  238 and it is recoded at 10 fps', b'There are only about 25 persons on average in each frame (Please refer to Table 1) The dataset provides the ROI for each video frame', b'By following the same setting with [4], we use frames from 601 to 1400 as training data, and the remaining 1200 frames are used as test data', b'This dataset does not satisfy assumptions that the crowd is evenly distributed', b'So we x the  of the density map', b'The intensities of pixels out of ROI is set to zero, and we also use ROI to revise the last convolution layer', b'Table 4 shows the results of our method and other methods on this dataset', b'The proposed MCNN model outperforms both the foreground segmentation based methods and CNN based method [33]', b'This indicates that our model can estimate not only images with extremely dense crowds but also images with relative sparse people', b'Table 4: Comparing results of different methods on the UCSD dataset', b'Method Kernel Ridge Regression [1] Ridge Regression [7] Gaussian Process Regression [4] Cumulative Attribute Regression [6] Zhang et al', b'[33] MCNN MAE MSE 7.45 2.16 2.25 7.82 7.97 2.24 6.86 2.07 3.31 1.60 1.07 1.35 3.5', b'The WorldExpo10 dataset WorldExpo10 crowd counting dataset was rstly introduced by Zhang et al', b'[33]', b'This dataset contains 1132 annotated video sequences which are captured by 108 surveillance cameras, all from Shanghai 2010 WorldExpo', b'The authors of [33] provided a total of 199,923 annotated pedestrians at the centers of their heads in 3980 frames', b'3380 frames are used in training data', b'Testing dataset includes ve different video sequences, and each video sequence contains 120 labeled frames', b'Five different regions of interest (ROI) are provided for the test scenes', b'In this dataset, the perspective maps are given', b'For fair Table 5: Mean absolute errors of the WorldExpo10 crowd counting dataset', b'Method LBP + RR Zhang et al', b'[33] MCNN Sence1 13.6 9.8 3.4 Sence2 59.8 14.1 20.6 Sence3 37.1 14.3 12.9 Sence4 21.8 22.2 13.0 Sence5 Average 23.4 3.7 8.1 31.0 12.9 11.6 comparison, we followed the work of [33], generated the density map according to perspective map with the relation  = 0.2  M (x), M (x) denotes that the number of pixels in the image representing one square meter at that location', b'To be consistent with [33], only ROI regions are considered in each test scene', b'So we modify the last convolution layer based on the ROI mask, namely, setting the neuron corresponding to the area out of ROI to zero', b'We use the same evaluation metric (MAE) suggested by the author of [33]', b'Table 5 reports the results of different methods in the ve test video sequences', b'Our method also achieves better performance than Finetuned Crowd CNN model [33] in terms of average MAE', b'3.6', b'Evaluation on transfer learning To demonstrate the generalizability of the learned model in our method, we test our method in the transfer learning setting by using the Part A of Shanghaitech dataset as the source domain and using the UCF CC 50 dataset as the target domain', b'Specically, we train a MCNNs model with data in the source domain', b'For the crowd counting task in the target domain, we conduct two settings, i.e., (i) no training samples in the target domain, and (ii) There are only a few samples in the target domain', b'For case (i), we directly use our model trained on Part A of Shanghaitech dataset for evaluation', b'For case (ii), we use the training samples in the target domain to netune the network', b'The performance of different settings is reported in Table 6', b'The accuracy differences between models trained on UCF CC 50 and Part A are similar (377.7 vs 397.7), which means the model trained on Part A is already good enough for the task on UCF CC 50', b'By netuning the last two layers of MCNN with training data on UCF CC 50, the accuracy can be greatly boosted (377.7 vs', b'295.1)', b'However, if the whole network is netuned rather than only the last two layers, the performance drops signicantly (295.1 vs 378.3), but still comparable (377.7 vs 378.31) with the MCNN model trained with the training data of the target domain', b'The performance gap between netuning the whole network and netuning the last couple of layers is perhaps due to the reason that we have limited training samples in the UCF CC 50 dataset', b'Finetuning the last two layers ensures that the output of the model is adapted to the target domain, and keeping the rst few layers of the model in596 tact ensures that good features/lters learned from adequate data in the source domain will be preserved', b'But if the whole network is netuned with inadequate data in the target domain, the learned model becomes similar to that learned with only the training data in the target domain', b'Hence the performance degrades to that of the model learned in the latter case', b'Table 6: Transfer learning across datasets', b'MCNN w/o transfer means we train the MCNN using the training data in UCF CC 50 only, and data from the source domain are not used', b'MCNN trained on Part A means we do not use the training data in the target domain to netune the MCNN trained in the source domain', b'Method MCNN w/o transfer MAE MSE 509.1 377.7 MCNN trained on Part A 397.7 624.1 Finetune the whole MCNN 378.3 594.6 490.23 295.1 Finetune the last two layers 4', b'Conclusion In this paper, we have proposed a Multicolumn Convolution Neural Network which can estimate crowd number accurately in a single image from almost any perspective', b'To better evaluate performances of crowd counting methods under practical conditions, we have collected and labelled a new dataset named Shanghaitech which consists of two parts with a total of 330,165 people annotated', b'This is the largest dataset so far in terms of the annotated heads for crowd counting', b'Our model outperforms the stateofart crowd counting methods on all datasets used for evaluation', b'Further, our model trained on a source domain can be easily transferred to a target domain by netuning only the last few layers of the trained model, which demonstrates good generalizability of the proposed model', b'5', b'Acknowledgement This work was supported by the Shanghai Pujiang Talent Program( No.15PJ1405700), and NSFC (No', b'61502304)', b'[19] Z', b'Lin and L', b'S', b'Davis', b'Shapebased human detection and segmentation via hierarchical parttemplate matching', b'Pattern Analysis and Machine Intelligence, 32(4):604618, 2010', b'[20] B', b'Liu and N', b'Vasconcelos', b'Bayesian model adaptation for crowd counts', b'In ICCV, 2015', b'[21] J', b'Long, E', b'Shelhamer, and T', b'Darrell', b'Fully convolutional networks for semantic segmentation', b'arXiv preprint arXiv:1411.4038, 2014', b'[22] A', b'Marana, L', b'd', b'F', b'Costa, R', b'Lotufo, and S', b'Velastin', b'On the efcacy of texture analysis for crowd monitoring', b'In International Symposium on Computer Graphics, Image Processing, and Vision, pages 354361', b'IEEE, 1998', b'[23] N', b'Paragios and V', b'Ramesh', b'A mrfbased approach for realtime subway monitoring', b'In CVPR, volume 1, pages I1034', b'IEEE, 2001', b'[24] V', b'Rabaud and S', b'Belongie', b'Counting crowded moving objects', b'In CVPR, volume 1, pages 705711', b'IEEE, 2006', b'[25] C', b'S', b'Regazzoni and A', b'Tesei', b'Distributed data fusion for realtime crowding estimation', b'Signal Processing, 53(1):47 63, 1996', b'[26] M', b'Rodriguez, I', b'Laptev, J', b'Sivic, and J.Y', b'Audibert', b'Densityaware person detection and tracking in crowds', b'In ICCV, pages 24232430', b'IEEE, 2011', b'[27] D', b'Ryan, S', b'Denman, C', b'Fookes, and S', b'Sridharan', b'Crowd counting using multiple local features', b'In Digital Image Computing: Techniques and Applications, pages 8188', b'IEEE, 2009', b'[28] K', b'Tota and H', b'Idrees', b'Counting in dense crowds using deep features', b'[29] P', b'Viola, M', b'J', b'Jones, and D', b'Snow', b'Detecting pedestrians using patterns of motion and appearance', b'International Journal of Computer Vision, 63(2):153161, 2005', b'[30] M', b'Wang and X', b'Wang', b'Automatic adaptation of a generic pedestrian detector to a specic trafc scene', b'In CVPR, pages 34013408', b'IEEE, 2011', b'[31] B', b'Wu and R', b'Nevatia', b'Detection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors', b'In ICCV, volume 1, pages 9097', b'IEEE, 2005', b'[32] M', b'D', b'Zeiler, M', b'Ranzato, R', b'Monga, M', b'Mao, K', b'Yang, Q', b'V', b'Le, P', b'Nguyen, A', b'Senior, V', b'Vanhoucke, and J', b'Dean', b'On rectied linear units for speech processing', b'In ICASSP, pages 35173521', b'IEEE, 2013', b'[33] C', b'Zhang, H', b'Li, X', b'Wang, and X', b'Yang', b'Crossscene crowd counting via deep convolutional neural networks', b'In CVPR, 2015', b'[34] T', b'Zhao, R', b'Nevatia, and B', b'Wu', b'Segmentation and tracking of multiple humans in crowded environments', b'Pattern Analysis and Machine Intelligence, 30(7):11981211, 2008', b'References [1] S', b'An, W', b'Liu, and S', b'Venkatesh', b'Face recognition using kernel ridge regression', b'In CVPR, pages 17', b'IEEE, 2007', b'[2] A', b'Bansal and K', b'Venkatesh', b'People counting in high density crowds from still images', b'arXiv preprint arXiv:1507.08445, 2015', b'[3] G', b'J', b'Brostow and R', b'Cipolla', b'Unsupervised bayesian detection of independent motion in crowds', b'In CVPR, volume 1, pages 594601', b'IEEE, 2006', b'[4] A', b'B', b'Chan, Z.S', b'J', b'Liang, and N', b'Vasconcelos', b'Privacy preserving crowd monitoring: Counting people without people models or tracking', b'In CVPR, pages 17', b'IEEE, 2008', b'[5] A', b'B', b'Chan and N', b'Vasconcelos', b'Bayesian poisson regression for crowd counting', b'In ICCV, pages 545551', b'IEEE, 2009', b'[6] K', b'Chen, S', b'Gong, T', b'Xiang, and C', b'C', b'Loy', b'Cumulative attribute space for age and crowd density estimation', b'In CVPR, pages 24672474', b'IEEE, 2013', b'[7] K', b'Chen, C', b'C', b'Loy, S', b'Gong, and T', b'Xiang', b'Feature mining for localised crowd counting', b'In BMVC, volume 1, page 3, 2012', b'[8] D', b'Ciresan, U', b'Meier, and J', b'Schmidhuber', b'Multicolumn In CVPR, deep neural networks for image classication', b'pages 36423649', b'IEEE, 2012', b'[9] K', b'Fukushima', b'Neocognitron: A selforganizing neural network model for a mechanism of pattern recognition unaffected by shift in position', b'Biological cybernetics, 36(4):193 202, 1980', b'[10] W', b'Ge and R', b'T', b'Collins', b'Marked point processes for crowd counting', b'In CVPR, pages 29132920', b'IEEE, 2009', b'[11] G', b'Hinton, S', b'Osindero, and Y', b'Teh', b'A fast learning algorithm for deep belief nets', b'NEURAL COMPUT, 18(7):15271554, 2006', b'[12] H', b'Idrees, I', b'Saleemi, C', b'Seibert, and M', b'Shah', b'Multisource In multiscale counting in extremely dense crowd images', b'CVPR, pages 25472554', b'IEEE, 2013', b'[13] H', b'Idrees, K', b'Soomro, and M', b'Shah', b'Detecting humans in dense crowds using locallyconsistent scale prior and global occlusion reasoning', b'Pattern Analysis and Machine Intelligence, 2005', b'[14] Y', b'Jia, E', b'Shelhamer, J', b'Donahue, S', b'Karayev, J', b'Long, R', b'Girshick, S', b'Guadarrama, and T', b'Darrell', b'Caffe: Convolutional architecture for fast feature embedding', b'arXiv preprint arXiv:1408.5093, 2014', b'[15] D', b'Kong, D', b'Gray, and H', b'Tao', b'Counting pedestrians in crowds using viewpoint invariant training', b'In BMVC', b'Citeseer, 2005', b'[16] Y', b'LeCun, L', b'Bottou, Y', b'Bengio, and P', b'Haffner', b'Gradientbased learning applied to document recognition', b'Proceedings of the IEEE, 86(11):22782324, 1998', b'[17] V', b'Lempitsky and A', b'Zisserman', b'Learning to count objects In Advances in Neural Information Processing in images', b'Systems, pages 13241332, 2010', b'[18] M', b'Li, Z', b'Zhang, K', b'Huang, and T', b'Tan', b'Estimating the number of people in crowded scenes by mid based foreground segmentation and headshoulder detection', b'In ICPR, pages 14', b'IEEE, 2008', b'597']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jun 27 18:55:35 2019\n",
    "\n",
    "@author: JM\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from tkinter.filedialog import askopenfilename \n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter,resolve1\n",
    "from pdfminer.pdfdevice import PDFDevice, TagExtractor\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.cmapdb import CMapDB\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.image import ImageWriter\n",
    "\n",
    "\n",
    "def isExistFile(path):\n",
    "    file_name = path.split('/')[-1]\n",
    "    \n",
    "    for i in os.listdir(\".\"):\n",
    "        if file_name == i:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def pdf2txt(input_path):\n",
    "    '''\n",
    "    input_path : str, PDF File\n",
    "    \n",
    "    =============================\n",
    "    \n",
    "    return : str, text File path\n",
    "    '''\n",
    "    \n",
    "    # input\n",
    "    password=''\n",
    "    pagenos=set()\n",
    "    maxpages=0\n",
    "    \n",
    "    # output\n",
    "    imagewriter = None\n",
    "    rotation = 0\n",
    "    codec = 'UTF-8'\n",
    "    pageno = 1\n",
    "    scale = 1\n",
    "    caching = True\n",
    "    showpageno = True\n",
    "    laparams = LAParams()\n",
    "    \n",
    "    infp = open(input_path,\"rb\")\n",
    "    \n",
    "    output_path = input_path[:-4] + '_trans.txt'\n",
    "    \n",
    "    outfp = open(output_path,\"w\",encoding='UTF8')\n",
    "    \n",
    "    #page total num\n",
    "    parser = PDFParser(infp)\n",
    "    document = PDFDocument(parser)\n",
    "    page_total_num = resolve1(document.catalog['Pages'])['Count']\n",
    "    \n",
    "    #\n",
    "    rsrcmgr = PDFResourceManager(caching=caching)\n",
    "\n",
    "    # pdf -> text converter\n",
    "    device = TextConverter(rsrcmgr,\n",
    "                           outfp,\n",
    "                           codec=codec,\n",
    "                           laparams=laparams, \n",
    "                           imagewriter=imagewriter)\n",
    "\n",
    "    # pdf -> text interpreter\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr,device)\n",
    "    \n",
    "    # pdf -> text start\n",
    "    with tqdm(total=page_total_num) as pbar:\n",
    "        for page in PDFPage.get_pages(infp,\n",
    "                                      pagenos,\n",
    "                                      maxpages,\n",
    "                                      password=password,\n",
    "                                      caching=caching,\n",
    "                                      check_extractable=True):\n",
    "\n",
    "            page.rotate = (page.rotate+rotation) % 360     \n",
    "            interpreter.process_page(page)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    print('==== success ====')\n",
    "\n",
    "    outfp.close()\n",
    "    infp.close()\n",
    "    \n",
    "    return output_path\n",
    "    \n",
    "def clean_text(path):\n",
    "    '''\n",
    "    path : str, text File Path\n",
    "    \n",
    "    \n",
    "    ===========================\n",
    "    \n",
    "    return : list, sentences\n",
    "    '''\n",
    "    \n",
    "    f = open(path,\"rb\")\n",
    "    line_list = []\n",
    "    \n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        line_list.append(line)\n",
    "        if not line: break\n",
    "    \n",
    "    # remove nextline\n",
    "    word = b\" \".join(line_list).split()\n",
    "    sentences = b\" \".join(word)\n",
    "    \n",
    "    \n",
    "    # remove ASCII\n",
    "    # define pattern \n",
    "    pattern = re.compile(b\"[\\x80-\\xff]\")\n",
    "    sentences = re.sub(pattern,b\"\",sentences)\n",
    "    \n",
    "    sentences = sentences.split(b\". \")\n",
    "    \n",
    "    cleaned_txt = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.replace(b\"- \",b'')\n",
    "        sentence = sentence.replace(b\"-\",b'')\n",
    "        cleaned_txt.append(sentence)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    os.remove(path)\n",
    "    \n",
    "    return cleaned_txt\n",
    "\n",
    "txt_path = pdf2txt('C:/Users/JM/Desktop/MCNN.pdf')\n",
    "cleaned_text = clean_text(txt_path)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Program Files\\\\Java\\\\jre7\\\\bin\\\\client\\\\jvm.dll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-91a316daa8d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKomoran\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mkomoran\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKomoran\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\konlpy\\tag\\_komoran.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, jvmpath, userdic, modelpath, max_heap_size)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjvmpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserdic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_heap_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misJVMStarted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mjvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_jvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjvmpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_heap_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodelpath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\konlpy\\jvm.py\u001b[0m in \u001b[0;36minit_jvm\u001b[1;34m(jvmpath, max_heap_size)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mclasspath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpathsep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfolder_suffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mjvmpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjvmpath\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetDefaultJVMPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# NOTE: Temporary patch for Issue #76. Erase when possible.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\jpype\\_core.py\u001b[0m in \u001b[0;36mget_default_jvm_path\u001b[1;34m()\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mjvmpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdetermined\u001b[0m \u001b[0mby\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mjpype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetDefaultJVMPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m     \u001b[0mParameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m      \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOptional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mArguments\u001b[0m \u001b[0mto\u001b[0m \u001b[0mgive\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mJVM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0margument\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mJVM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\jpype\\_jvmfinder.py\u001b[0m in \u001b[0;36mget_jvm_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjvm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \"\"\"\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\jpype\\_windows.py\u001b[0m in \u001b[0;36mcheck\u001b[1;34m(self, jvm)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# Library file name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"jvm.dll\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# Search methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\jpype\\_windows.py\u001b[0m in \u001b[0;36m_checkJVMArch\u001b[1;34m(jvmPath)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mIMAGE_FILE_MACHINE_AMD64\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m34404\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mis64\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxsize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjvmPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Program Files\\\\Java\\\\jre7\\\\bin\\\\client\\\\jvm.dll'"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
